{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from numpy.matlib import repmat\n",
    "from scipy.spatial import distance_matrix as ssdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import glob\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import carla \n",
    "\n",
    "from identify_instances import COLORMAP\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Folder Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "town_str = 'Town02' # 01, 02, 04\n",
    "datadir = '../../%s_demo/' % town_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image and Snapshot Preparation Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rgb_image(imgloc):\n",
    "    im_rgb = Image.open(imgloc).convert('RGB')\n",
    "    return np.array(im_rgb)\n",
    "\n",
    "def process_seg_image(imgloc):\n",
    "    im_seg = Image.open(imgloc)\n",
    "    return np.array(im_seg)[:,:,0]\n",
    "\n",
    "def process_depth_image(imgloc):\n",
    "    im_depth = Image.open(imgloc)\n",
    "    im_depth = np.array(im_depth)[:,:,:3].astype(np.float)\n",
    "    \n",
    "    # formula: \n",
    "    # norm_depth = (r + g * 256 + b * 256**2) / (256**3 - 1)\n",
    "    # norm_depth = 1 => 1 km depth in optical axis\n",
    "\n",
    "    # https://github.com/carla-simulator/carla/issues/2817\n",
    "    norm_depth = np.dot(im_depth, [1.0, 256.0, 65536.0])\n",
    "    norm_depth /= 16777215.0\n",
    "    \n",
    "    im_depth = norm_depth * 1000. # meters, max resolution is 1 km\n",
    "    return im_depth\n",
    "\n",
    "def process_snapshot_json(jsonfile):\n",
    "    data_dict = json.load(open(jsonfile, 'r'))\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Cloud Processing Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_camera_point_cloud(seg_img, depth_img, cx, cy, f):\n",
    "    # Vectorized implemention courtesy of:\n",
    "    # https://github.com/carla-simulator/carla/blob/cffd944c466026a1fca1ea1f711b86c96117acee/PythonClient/carla/image_converter.py#L111\n",
    "    K = np.array([[f, 0., cx], \\\n",
    "                  [0., f, cy], \\\n",
    "                  [0., 0., 1]], dtype=np.float)\n",
    "    \n",
    "    height, width = seg_img.shape\n",
    "    \n",
    "    us = repmat(np.r_[0:width:1], height, 1)\n",
    "    vs = repmat(np.c_[0:height:1], 1, width)\n",
    "    \n",
    "    p2d_list = np.array([us.flatten(), vs.flatten(), np.ones(height*width)], dtype=np.float) # 3 by HW, homogenous 2d point representation    \n",
    "    p3d_list = depth_img.flatten() * np.dot(np.linalg.inv(K), p2d_list) # 3 by HW, XYZ camera 3D coordinates\n",
    "    \n",
    "    color_list = np.array([COLORMAP[col] for col in seg_img.flatten()], dtype=np.float) # i.e. 0.0 -> 255.0\n",
    "    seg_lbl = seg_img.flatten().reshape(width*height, 1)\n",
    "    \n",
    "    point_cloud_list = np.concatenate((p3d_list.T, seg_lbl, color_list), axis=1) # [X, Y, Z, seg_label, col_r, col_g, col_b]\n",
    "    return point_cloud_list\n",
    "\n",
    "def threshold_point_cloud_depth(point_cloud_list, max_depth=100.):\n",
    "    # Remove points with depth (Z) greater than max_depth.\n",
    "    keep_inds = point_cloud_list[:,2] < max_depth\n",
    "    return point_cloud_list[keep_inds,:]\n",
    "\n",
    "def threshold_point_cloud_seg_class(point_cloud_list, seg_classes=[4, 10]):\n",
    "    # Remove points that have a segmentation label not in seg_classes.\n",
    "    keep_inds = np.zeros(point_cloud_list.shape[0], dtype=np.bool)\n",
    "    \n",
    "    for seg_lbl in seg_classes:\n",
    "        match_inds = point_cloud_list[:,3] == seg_lbl\n",
    "        keep_inds += match_inds # OR\n",
    "    \n",
    "    return point_cloud_list[keep_inds,:]\n",
    "    \n",
    "\"\"\"\n",
    "def project_camera_points_by_class(seg_img, depth_img, fov, seg_classes = [4]):\n",
    "    # Converts instances with label in seg_classes from pixel to 3D.  Final transform\n",
    "    # used to convert from camera coordinates to another 3D world coordinate system.\n",
    "    \n",
    "    height, width = seg_img.shape\n",
    "    cx = width / 2.\n",
    "    cy = height / 2.\n",
    "    f  = width / ( 2. * np.tan(np.radians(fov/2.)) )    \n",
    "    cam_3d_points = []\n",
    "    \n",
    "    for seg_lbl in seg_classes:\n",
    "        seg_inds = np.argwhere(seg_img == seg_lbl)\n",
    "        seg_color = COLORMAP[seg_lbl]\n",
    "        \n",
    "        for (v,u) in seg_inds:            \n",
    "            Z_cam = depth_img[v,u]\n",
    "            X_cam = (u - cx + 0.5) / f * Z_cam\n",
    "            Y_cam = (v - cy + 0.5) / f * Z_cam\n",
    "            \n",
    "            \n",
    "            \n",
    "            if final_transform:\n",
    "                X_unreal_local, Y_unreal_local, Z_unreal_local = \\\n",
    "                    convert_pinhole_to_unreal_3d_coords([X_cam, Y_cam, Z_cam])\n",
    "                \n",
    "                loc_unreal_global = local_to_world_frame(carla.Location(x=X_unreal_local,\n",
    "                                                                        y=Y_unreal_local,\n",
    "                                                                        z=Z_unreal_local), \\\n",
    "                                                         final_transform)\n",
    "                \n",
    "                cam_3d_points.append([loc_unreal_global[0],\n",
    "                                      loc_unreal_global[1],\n",
    "                                      loc_unreal_global[2],\n",
    "                                      seg_color[0],\n",
    "                                      seg_color[1],\n",
    "                                      seg_color[2]])\n",
    "            else:\n",
    "                cam_3d_points.append([X_cam, Y_cam, Z_cam, \\\n",
    "                                      seg_color[0], seg_color[1], seg_color[2]])\n",
    "    return np.array(cam_3d_points)\n",
    "\n",
    "def project_camera_point_cloud(seg_img, depth_img, fov):\n",
    "    # This function converts a scene represented by segmentation and depth images\n",
    "    # to a point cloud array.\n",
    "    \n",
    "    height, width = seg_img.shape\n",
    "    cx = width / 2.\n",
    "    cy = height / 2.\n",
    "    f  = width / ( 2. * np.tan(np.radians(fov/2.)) )\n",
    "    \n",
    "    cam_point_cloud = []\n",
    "    \n",
    "    # TODO: vectorize and prune elements with 0 label at the end.\n",
    "    # reference: https://github.com/carla-simulator/carla/blob/cffd944c466026a1fca1ea1f711b86c96117acee/PythonClient/carla/image_converter.py#L148\n",
    "    for v in range(int(cy), height, 2):\n",
    "        for u in range(0, width, 1):            \n",
    "            lbl = seg_img[v,u]\n",
    "            if lbl == 0:\n",
    "                continue\n",
    "                \n",
    "            Z_cam = depth_img[v,u]\n",
    "            X_cam = (u - cx + 0.5) / f * Z_cam\n",
    "            Y_cam = (v - cy + 0.5) / f * Z_cam\n",
    "            \n",
    "            color = COLORMAP[lbl]\n",
    "            \n",
    "            cam_point_cloud.append([X_cam, Y_cam, Z_cam, color[0], color[1], color[2]])\n",
    "    \n",
    "    return np.array(cam_point_cloud)\n",
    "\"\"\"\n",
    "\n",
    "def downsample_and_project_point_cloud(point_cloud, xmin_m, xmax_m, zmin_m, zmax_m, grid_res):\n",
    "    # Rough top-view projection of the point cloud.  X,Z are in the camera coordinate system.\n",
    "    \n",
    "    # Assuming a grid with grid resolution and limits (*min, *max), this returns the center of each cell\n",
    "    # in the grid.  This can then be used to populate the cell with pixel RGB values based on a\n",
    "    # nearest neighbor downsampling.\n",
    "    \n",
    "    # Grid Coordinates in X and Z.\n",
    "    xrow = np.r_[xmin_m + grid_res / 2. : xmax_m : grid_res] # N\n",
    "    zcol = np.c_[zmin_m + grid_res / 2. : zmax_m : grid_res] # M\n",
    "    \n",
    "    # Number of Grid Coordinates by Axis (N, M).\n",
    "    N_x = np.size(xrow)\n",
    "    M_z = np.size(zcol)\n",
    "\n",
    "    # Top View projection Image.\n",
    "    projection = np.zeros((M_z, N_x, 3), dtype=np.uint8) # M by N\n",
    "    \n",
    "    # Grid XZ points.\n",
    "    xs = repmat(xrow, M_z, 1) # M by N\n",
    "    zs = repmat(zcol, 1, N_x) # M by N\n",
    "    \n",
    "    # XZ Grid and Point Cloud List.\n",
    "    xz_grid_list = np.column_stack( (xs.flatten(), zs.flatten()) ) # MN by 2\n",
    "    xz_point_cloud = point_cloud[:, [0,2]]\n",
    "    \n",
    "    # Euclidean Distance Matrix.\n",
    "    # # dm[i,j] = dist(xz_grid_list[i], xz_point_cloud[j])\n",
    "    min_inds = np.ones(xz_grid_list.shape[0]) * -1\n",
    "    \n",
    "    print('SSDM Started')\n",
    "    \n",
    "    for st_ind in np.arange(0, xz_grid_list.shape[0], 1000):\n",
    "        end_ind = min(xz_grid_list.shape[0], st_ind + 1000)\n",
    "        dm = ssdm(xz_grid_list[st_ind:end_ind,:], xz_point_cloud) \n",
    "        \n",
    "        min_dist_subset = np.amin(dm, axis=1)   # dist from xz grid location to nearest point cloud point\n",
    "        min_inds_subset = np.argmin(dm, axis=1) # index of nearest point cloud point\n",
    "        min_inds_subset[min_dist_subset > grid_res] = -1 # no point nearby\n",
    "        min_inds[st_ind:end_ind] = min_inds_subset        \n",
    "        \n",
    "    print('SSDM Done')\n",
    "    \n",
    "    # Populate projection image with colors.\n",
    "    min_inds = min_inds.reshape( (M_z, N_x) )\n",
    "    for ind_z in range(min_inds.shape[0]):\n",
    "        for ind_x in range(min_inds.shape[1]):\n",
    "            if min_inds[ind_z, ind_x] < 0:\n",
    "                continue\n",
    "            else:\n",
    "                pc_ind = int(min_inds[ind_z, ind_x])\n",
    "                projection[ind_z, ind_x] = point_cloud[ pc_ind, 4:].astype(np.uint8)\n",
    "    \n",
    "    print('For Loop Done')\n",
    "    \n",
    "    return projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot 3D Elements Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_orientation_arrow(origin, yaw, length, color='c', width=0.1):\n",
    "    x, y, = origin\n",
    "    dx = length * np.cos( np.radians(yaw) )\n",
    "    dy = length * np.sin( np.radians(yaw) )\n",
    "    plt.arrow(x, y, dx, dy, color=color, width=width)\n",
    "\n",
    "def plot_text(origin, text, color='c'):\n",
    "    x, y = origin\n",
    "    plt.text(x, y + 0.2, text, color=color, fontsize='medium')\n",
    "    \n",
    "    \n",
    "def plot_bbox_world_vertices(world_vertices):\n",
    "    bottom_left_back  = world_vertices[0]\n",
    "    bottom_right_back = world_vertices[2]\n",
    "    bottom_right_fwd  = world_vertices[6]\n",
    "    bottom_left_fwd   = world_vertices[4]\n",
    "    \n",
    "    plt.plot([bottom_left_back['x'], bottom_right_back['x']], \\\n",
    "             [bottom_left_back['y'], bottom_right_back['y']], 'k')\n",
    "    \n",
    "    plt.plot([bottom_right_back['x'], bottom_right_fwd['x']], \\\n",
    "             [bottom_right_back['y'], bottom_right_fwd['y']], 'r')\n",
    "    \n",
    "    plt.plot([bottom_right_fwd['x'], bottom_left_fwd['x']], \\\n",
    "             [bottom_right_fwd['y'], bottom_left_fwd['y']],'g')\n",
    "    \n",
    "    plt.plot([bottom_left_fwd['x'], bottom_left_back['x']], \\\n",
    "             [bottom_left_fwd['y'], bottom_left_back['y']], 'b')\n",
    "    \n",
    "    [plt.plot(p['x'], p['y'], 'k.') for p in world_vertices]\n",
    "\n",
    "def get_rotation_matrix(transform):\n",
    "    # Source: https://github.com/carla-simulator/carla/blob/3926fec5fb448b6c52b5a2d5e73c790e8736cc11/LibCarla/source/carla/geom/Transform.h#L81\n",
    "    # Also helpful: https://github.com/carla-simulator/carla/blob/dev/LibCarla/source/carla/geom/Math.cpp\n",
    "    # This is implemented in the latest Carla versions but not 0.9.9 :(\n",
    "    # Thus replicated here for convenience.\n",
    "    # NOTE: Unreal coordinate system (LHS).\n",
    "    \n",
    "    cp = np.cos(np.radians(transform.rotation.pitch))\n",
    "    sp = np.sin(np.radians(transform.rotation.pitch))\n",
    "    \n",
    "    cr = np.cos(np.radians(transform.rotation.roll))\n",
    "    sr = np.sin(np.radians(transform.rotation.roll))\n",
    "    \n",
    "    cy = np.cos(np.radians(transform.rotation.yaw))\n",
    "    sy = np.sin(np.radians(transform.rotation.yaw))\n",
    "    \n",
    "    rotation_matrix = np.array([\\\n",
    "        [cp * cy,  cy * sp * sr - sy * cr, -cy * sp * cr - sy * sr], \\\n",
    "        [cp * sy,  sy * sp * sr + cy * cr, -sy * sp * cr + cy * sr], \\\n",
    "        [sp     , -cp * sr              ,   cp * cr]])\n",
    "    return rotation_matrix\n",
    "    \n",
    "def world_to_local_frame(world_loc, transform):\n",
    "    # NOTE: Unreal coordinate system (LHS).\n",
    "    if type(world_loc) == carla.Location:\n",
    "        diff_loc = world_loc - transform.location\n",
    "        diff_arr = np.array([diff_loc.x, diff_loc.y, diff_loc.z]).reshape(3,1)\n",
    "    else:\n",
    "        trans_arr = np.array([transform.location.x, transform.location.y, transform.location.z]).reshape(3,1)\n",
    "        diff_arr = world_loc - trans_arr        \n",
    "    \n",
    "    inv_rotation_matrix = get_rotation_matrix(transform).T\n",
    "    local_loc_arr = inv_rotation_matrix @ diff_arr\n",
    "   \n",
    "    return local_loc_arr\n",
    "\n",
    "def local_to_world_frame(local_loc, transform):\n",
    "    # NOTE: Unreal coordinate system (LHS).\n",
    "    rotation_matrix = get_rotation_matrix(transform)\n",
    "    trans_arr = np.array([transform.location.x, transform.location.y, transform.location.z]).reshape(3,1)\n",
    "    \n",
    "    if type(local_loc) == carla.Location:\n",
    "        local_arr = np.array([local_loc.x, local_loc.y, local_loc.z]).reshape(3,1)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    rot_vec = rotation_matrix @ local_arr\n",
    "    world_loc_arr = rot_vec + trans_arr\n",
    "    \n",
    "    return world_loc_arr\n",
    "    \n",
    "def convert_pinhole_to_unreal_3d_coords(coord_pinhole_3d):\n",
    "    \"\"\" Assumptions:\n",
    "        - coord_pinhole_3d are the camera coordinates used by the pinhole model (right-handed):\n",
    "          X_cam=right, Y_cam=down, Z_cam=fwd\n",
    "        - Unreal uses a left-handed coordinate system, according to the following link:\n",
    "        http://www.aclockworkberry.com/world-coordinate-systems-in-3ds-max-unity-and-unreal-engine/\n",
    "          X_unreal = fwd, Y_unreal=right, Z_unreal=up\n",
    "    \"\"\"\n",
    "    \n",
    "    X_cam, Y_cam, Z_cam = coord_pinhole_3d\n",
    "    return Z_cam, X_cam, -Y_cam\n",
    "\n",
    "def project_world_boxes_in_camera_frame(world_bbox, camera_transform, width, height, fov):    \n",
    "    cx = width / 2.\n",
    "    cy = height / 2.\n",
    "    f  = width / ( 2. * np.tan(np.radians(fov/2.)) )\n",
    "    \n",
    "    us = []; vs = []\n",
    "    for ind, world_vertex in enumerate(world_bbox['world_vertices']):\n",
    "        print(ind)\n",
    "        print('\\tX: ', world_vertex['x'], \\\n",
    "              'Y: ', world_vertex['y'], \\\n",
    "              'Z: ', world_vertex['z'])\n",
    "        \n",
    "        local_loc = world_to_local_frame(carla.Location(x=world_vertex['x'], \\\n",
    "                                                        y=world_vertex['y'], \\\n",
    "                                                        z=world_vertex['z']), \\\n",
    "                                         camera_transform)\n",
    "        \n",
    "        # TODO: inverse of convert_pinhole..., maybe make a function later.\n",
    "        X_cam, Y_cam, Z_cam = local_loc[1], -local_loc[2], local_loc[0]\n",
    "        \n",
    "        print('\\tX_cam: ', X_cam, \\\n",
    "              'Y_cam: ', Y_cam, \\\n",
    "              'Z_cam: ', Z_cam)\n",
    "        \n",
    "        u = f * X_cam / Z_cam + cx\n",
    "        v = f * Y_cam / Z_cam + cy\n",
    "        us.append(u)\n",
    "        vs.append(v)\n",
    "        \n",
    "        print('\\t pixel:', u, v)\n",
    "    return us, vs\n",
    "#     print('\\t', np.min(us), np.min(vs), np.max(us), np.max(vs), '\\n')\n",
    "#     return np.min(us), np.min(vs), np.max(us), np.max(vs) # umin, vmin, umax, vmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length Consistency Check: True with 50 elements each.\n"
     ]
    }
   ],
   "source": [
    "data_dicts  = [process_snapshot_json(x) for x in glob.glob(datadir + '/snapshot/*.json')]\n",
    "\n",
    "center_imgs = [[x for x in glob.glob(datadir + '/rgb_center/*.png')], \\\n",
    "               [x for x in glob.glob(datadir + '/seg_center/*.png')], \\\n",
    "               [x for x in glob.glob(datadir + '/depth_center/*.png')]]\n",
    "\n",
    "right_imgs  = [[x for x in glob.glob(datadir + '/rgb_right/*.png')], \\\n",
    "               [x for x in glob.glob(datadir + '/seg_right/*.png')], \\\n",
    "               [x for x in glob.glob(datadir + '/depth_right/*.png')]]\n",
    "\n",
    "left_imgs   = [[x for x in glob.glob(datadir + '/rgb_left/*.png')], \\\n",
    "               [x for x in glob.glob(datadir + '/seg_left/*.png')], \\\n",
    "               [x for x in glob.glob(datadir + '/depth_left/*.png')]]\n",
    "\n",
    "far_imgs    = [[x for x in glob.glob(datadir + '/rgb_far/*.png')], \\\n",
    "               [x for x in glob.glob(datadir + '/seg_far/*.png')], \\\n",
    "               [x for x in glob.glob(datadir + '/depth_far/*.png')]]\n",
    "\n",
    "dataset_lengths = [len(data_dicts)]\n",
    "for img_dict in [center_imgs, right_imgs, left_imgs, far_imgs]:\n",
    "    dataset_lengths.extend([len(x) for x in img_dict])\n",
    "length_consistency = np.all([x == dataset_lengths[0] for x in dataset_lengths])\n",
    "print('Length Consistency Check: %s with %d elements each.' % (length_consistency, dataset_lengths[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[print(k) for k in data_dicts[0].keys()]\n",
    "\n",
    "def load_local_scene(data_dict, ego_name='hero'):\n",
    "    ego_dict           = {}\n",
    "    other_vehicle_dict = {}\n",
    "    walker_dict        = {}\n",
    "    traffic_light_dict = {}\n",
    "    traffic_stop_dict  = {}\n",
    "    traffic_yield_dict = {}\n",
    "    traffic_speed_dict = {}\n",
    "    \n",
    "    for (k, v) in data_dict.items():\n",
    "        if k in ['id', 'frame', 'delta_seconds', 'elapsed_seconds']:\n",
    "            continue\n",
    "        \n",
    "        actor_id = int(k.split('_')[-1])\n",
    "        \n",
    "        if 'traffic_light' in k:\n",
    "            traffic_light_dict[actor_id] = v\n",
    "        elif 'traffic.stop' in k:\n",
    "            traffic_stop_dict[actor_id]  = v\n",
    "        elif 'traffic.yield' in k:\n",
    "            traffic_yield_dict[actor_id] = v\n",
    "        elif 'traffic.speed_limit' in k:\n",
    "            speed = int(k.split('speed_limit.')[-1].split('_')[0])\n",
    "            traffic_speed_dict[actor_id] = v\n",
    "            traffic_speed_dict[actor_id]['speed'] = speed\n",
    "        elif 'vehicle' in k:\n",
    "            if v['role_name'] == 'hero':\n",
    "                ego_dict = v\n",
    "                ego_dict['make'] = k.split('vehicle.')[-1].split('_')[0]\n",
    "            else:\n",
    "                other_vehicle_dict[actor_id] = v\n",
    "                other_vehicle_dict[actor_id]['make'] = k.split('vehicle.')[-1].split('_')[0]\n",
    "        elif 'walker' in k:\n",
    "            walker_dict[actor_id] = v\n",
    "    \n",
    "    scene_dict = {}\n",
    "    scene_dict['ego'] = ego_dict\n",
    "    scene_dict['other_vehicles'] = other_vehicle_dict\n",
    "    scene_dict['walkers'] = walker_dict\n",
    "    scene_dict['traffic_lights'] = traffic_light_dict\n",
    "    scene_dict['stop_signs'] = traffic_stop_dict\n",
    "    scene_dict['yield_signs'] = traffic_yield_dict\n",
    "    scene_dict['speed_signs'] = traffic_speed_dict\n",
    "    \n",
    "    return scene_dict\n",
    "\n",
    "def convert_carla_location(carla_loc_dict):\n",
    "    # https://github.com/carla-simulator/ros-bridge/blob/0556f05d086b6d43c6f91f073d116654b3c84969/carla_common/src/carla_common/transforms.py#L51\n",
    "    x, y, z = [carla_loc_dict[k] for k in ['x', 'y', 'z']]\n",
    "    return np.array([x, -y, z]).reshape(3,1)\n",
    "\n",
    "def convert_carla_rotation(carla_rot_dict):\n",
    "    # https://github.com/carla-simulator/ros-bridge/blob/0556f05d086b6d43c6f91f073d116654b3c84969/carla_common/src/carla_common/transforms.py#L109\n",
    "    \n",
    "    r, p, y = [carla_rot_dict[k] for k in ['roll', 'pitch', 'yaw']]\n",
    "    return np.radians([r, -p, -y])\n",
    "\n",
    "def euler_to_rotation_matrix(r, p, y):\n",
    "    # This is using the ROS convention!  It rotates about x, then y, then z using static axes.\n",
    "    # R = R_z(yaw_ros) * R_y(pitch_ros) * R_x(roll_ros).\n",
    "    # Source: http://docs.ros.org/jade/api/tf/html/python/transformations.html (default = 'sxyz')\n",
    "    c = np.cos\n",
    "    s = np.sin\n",
    "    \n",
    "    Rx_r = np.array([[1, 0, 0], [0, c(r), -s(r)], [0, s(r), c(r)]], dtype=np.float)\n",
    "    Ry_p = np.array([[c(p), 0, s(p)], [0, 1, 0], [-s(p), 0, c(p)]], dtype=np.float)\n",
    "    Rz_y = np.array([[c(y), -s(y), 0], [s(y), c(y), 0], [0, 0, 1]], dtype=np.float)\n",
    "    \n",
    "    return Rz_y @ Ry_p @ Rx_r\n",
    "\n",
    "def local_to_world_frame(v3d_local, R, t):\n",
    "    # R: local -> world\n",
    "    # t: local's origin in world coordinates\n",
    "    return R @ v3d_local + t\n",
    "    \n",
    "\n",
    "def world_to_local_frame(v3d_world, R, t):\n",
    "    # R: local -> world\n",
    "    # t: local's origin in world coordinates\n",
    "    R_inv = R.T\n",
    "    t_inv = - R.T @ t\n",
    "    return R_inv @ v3d_world + t_inv\n",
    "    \n",
    "def plot_local_scene_ego_view(scene_dict):\n",
    "    # Notation: *_w => in a right-handed coordinate system corresponding to the world system\n",
    "    #           *_e => in a right-handed coordinate system corresponding to ego's reference\n",
    "    ego_loc_w = convert_carla_location(scene_dict['ego']['location'])\n",
    "    ego_rpy_w = convert_carla_rotation(scene_dict['ego']['rotation'])\n",
    "    ego_R_w = euler_to_rotation_matrix(*ego_rpy_w)\n",
    "    \n",
    "    plt.figure()    \n",
    "    \n",
    "    plt.arrow(0, 0, 10, 0, color='k')\n",
    "    plt.arrow(0, 0, 0, 10, color='g')\n",
    "    \n",
    "    for (k,v) in scene_dict['other_vehicles'].items():\n",
    "        veh_loc_w = convert_carla_location(v['location'])\n",
    "        veh_loc_e = world_to_local_frame(veh_loc_w, ego_R_w, ego_loc_w)        \n",
    "        \n",
    "        x_e, y_e, z_e = veh_loc_e\n",
    "        plt.plot(x_e, y_e, 'b.')\n",
    "        plt.text(x_e, y_e - 0.5, 'veh_%d' % v['id'], color='b')\n",
    "    \n",
    "    for (k,v) in scene_dict['walkers'].items():\n",
    "        ped_loc_w = convert_carla_location(v['location'])\n",
    "        ped_loc_e = world_to_local_frame(ped_loc_w, ego_R_w, ego_loc_w)        \n",
    "        \n",
    "        x_e, y_e, z_e = ped_loc_e\n",
    "        plt.plot(x_e, y_e, 'r.')\n",
    "        plt.text(x_e, y_e - 0.5, 'ped_%d' % v['id'], color='r')\n",
    "        \n",
    "    plt.xlim([-10, 75])\n",
    "    plt.ylim([-50, 50])\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "scene_dict = load_local_scene(data_dicts[25])\n",
    "plot_local_scene_ego_view(scene_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform(Location(x=97.897575, y=191.629959, z=1.856224), Rotation(pitch=-0.174955, yaw=-1.334015, roll=-0.032135))\n",
      "Projecting\n",
      "Downsampling\n"
     ]
    }
   ],
   "source": [
    "camera_arr =  center_imgs\n",
    "fov = 120.\n",
    "img_num    = 25\n",
    "camera_role_name = 'center'\n",
    "camera_transform = None\n",
    "\n",
    "for k,v in data_dicts[img_num].items():\n",
    "    if 'rgb' in k:\n",
    "        if camera_role_name == v['role_name']:\n",
    "            \n",
    "            loc = carla.Location(x = v['location']['x'],\n",
    "                                 y = v['location']['y'],\n",
    "                                z = v['location']['z'])\n",
    "            rot = carla.Rotation(pitch = v['rotation']['pitch'],\n",
    "                                 yaw   = v['rotation']['yaw'],\n",
    "                                 roll  = v['rotation']['roll'])\n",
    "            \n",
    "            camera_transform = carla.Transform(location=loc, rotation=rot)\n",
    "\n",
    "print(camera_transform)\n",
    "\n",
    "rgb_img   = process_rgb_image(camera_arr[0][img_num])\n",
    "seg_img   = process_seg_image(camera_arr[1][img_num])\n",
    "depth_img = process_depth_image(camera_arr[2][img_num])\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(131)\n",
    "plt.imshow(rgb_img)\n",
    "plt.subplot(132)\n",
    "plt.imshow(seg_img, cmap='jet')\n",
    "plt.subplot(133)\n",
    "plt.imshow(np.log(depth_img), cmap='gray')\n",
    "\n",
    "height, width = seg_img.shape\n",
    "cx = float(width / 2)\n",
    "cy = float(height / 2)\n",
    "f  = width / ( 2. * np.tan(np.radians(fov/2.)) )  \n",
    "\n",
    "plt.figure()\n",
    "print('Projecting')\n",
    "point_cloud = project_camera_point_cloud(seg_img, depth_img, cx, cy, f)\n",
    "point_cloud_ds = threshold_point_cloud_seg_class(point_cloud, seg_classes=[4, 10, 12])\n",
    "point_cloud_ds = threshold_point_cloud_depth(point_cloud_ds, max_depth=50.)\n",
    "print('Downsampling')\n",
    "# point_cloud = project_camera_points_by_class(seg_img, depth_img, fov, seg_classes=[4, 6, 7, 10, 12])\n",
    "# point_cloud = project_camera_point_cloud(seg_img, depth_img, fov)\n",
    "# for point in point_cloud:\n",
    "#     plt.plot(point[0], point[2], '.', color=tuple([float(x) / 255. for x in point[3:]]) )\n",
    "\n",
    "xmin_m = -10\n",
    "xmax_m =  10\n",
    "zmin_m =   0.\n",
    "zmax_m =  50.\n",
    "\n",
    "# Render as an image approach, Euclidean distance matrix approach not that fast.\n",
    "#grid_res = 0.2\n",
    "#point_cloud_top_view = downsample_and_project_point_cloud(point_cloud_ds, xmin_m, xmax_m, zmin_m, zmax_m, grid_res)           \n",
    "#plt.imshow(point_cloud_top_view, origin='lower')\n",
    "\n",
    "plt.scatter(point_cloud_ds[:, 0], point_cloud_ds[:,2], color=point_cloud_ds[:,4:]/255.)\n",
    "plt.ylim([zmin_m, zmax_m])\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# cam_points = project_camera_points(seg_img, depth_img, fov, seg_classes = [4, 10], final_transform=camera_transform)\n",
    "\n",
    "cam_orig   = camera_transform.transform(carla.Location(0, 0, 0))\n",
    "cam_x_axis = camera_transform.transform(carla.Location(75, 0, 0))\n",
    "cam_y_axis = camera_transform.transform(carla.Location(0, 75, 0))\n",
    "\n",
    "cam_x_axis_vg = local_to_world_frame(carla.Location(75, 0, 0), camera_transform)\n",
    "cam_y_axis_vg = local_to_world_frame(carla.Location(0, 75, 0), camera_transform)\n",
    "print('world: ', cam_x_axis_vg, cam_y_axis_vg)\n",
    "\n",
    "x_recon = world_to_local_frame(cam_x_axis_vg, camera_transform)\n",
    "y_recon = world_to_local_frame(cam_y_axis_vg, camera_transform)\n",
    "print('local: ', x_recon, y_recon)\n",
    "\n",
    "# for point in cam_points:\n",
    "#     plot_color = tuple( [float(x) / 255. for x in COLORMAP[point[3]]] )\n",
    "#     plt.plot(point[0], point[1], 'o', color=plot_color)\n",
    "\n",
    "    \n",
    "ego_x, ego_y = [0., 0.]\n",
    "\n",
    "for (k,v) in data_dicts[img_num].items():\n",
    "    \n",
    "    if 'sensor' in k:\n",
    "        [print(k, kk, vv) for (kk, vv) in v.items()]\n",
    "        \n",
    "    if 'vehicle' in k or 'walker' in k:\n",
    "        plot_bbox_world_vertices(v['bounding_box']['world_vertices'])\n",
    "        plot_orientation_arrow([v['location']['x'], v['location']['y']], \\\n",
    "                                v['rotation']['yaw'], \\\n",
    "                                1.0)\n",
    "        if v['role_name'] == 'pedestrian':\n",
    "            text = 'ped_%d' % v['id']\n",
    "            text_col  = 'c'\n",
    "        elif v['role_name'] == 'autopilot':\n",
    "            text = 'veh_%d' % v['id']\n",
    "            text_col  = 'b'\n",
    "        elif v['role_name'] == 'hero':\n",
    "            ego_x = v['location']['x']\n",
    "            ego_y = v['location']['y']\n",
    "            text = 'ego'\n",
    "            text_col  = 'r'\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected role: \" % v['role_name'])\n",
    "            \n",
    "        plot_text([v['location']['x'], v['location']['y']], text, color=text_col)\n",
    "    \n",
    "    \n",
    "plt.plot(camera_transform.location.x, camera_transform.location.y, 'bx')\n",
    "\n",
    "plt.plot(cam_x_axis.x, cam_x_axis.y, 'gx')\n",
    "plt.plot(cam_y_axis.x, cam_y_axis.y, 'yx')\n",
    "plt.plot(cam_x_axis_vg[0], cam_x_axis_vg[1], 'g+')\n",
    "plt.plot(cam_y_axis_vg[0], cam_y_axis_vg[1], 'y+')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "im_overlay = np.copy(rgb_img)\n",
    "# plt.imshow(rgb_img)\n",
    "\n",
    "for (k,v) in data_dicts[img_num].items():\n",
    "    if 'vehicle' in k or 'walker' in k:\n",
    "        if v['id'] in [264]:\n",
    "            pass\n",
    "        else:\n",
    "            continue\n",
    "        img_height, img_width, _ = rgb_img.shape        \n",
    "        us, vs = project_world_boxes_in_camera_frame(v['bounding_box'], camera_transform, img_width, img_height, fov)\n",
    "        for ind, (u, v) in enumerate(zip(us, vs)):\n",
    "            if u < 0 or v < 0 or u > img_width or v > img_height:\n",
    "                pass\n",
    "            else:\n",
    "                u = int(u); v = int(v)\n",
    "                cv2.circle(im_overlay, (u,v), 2,(255, 0, 0))\n",
    "                \n",
    "                \n",
    "#         umin, vmin, umax, vmax = project_world_boxes_in_camera_frame(v['bounding_box'], camera_transform, img_width, img_height, fov)\n",
    "        \n",
    "#         if umax < 0 or vmax < 0:\n",
    "#             pass # left/above image extent\n",
    "#         elif umin > img_width or vmin > img_height:\n",
    "#             pass # right/below image extent\n",
    "#         else:\n",
    "#             umin = max(0, int(umin))\n",
    "#             vmin = max(0, int(vmin))\n",
    "#             umax = min(img_width,  int(umax))\n",
    "#             vmax = min(img_height, int(vmax))\n",
    "            \n",
    "#             if v['role_name'] == 'pedestrian':\n",
    "#                 text = 'ped'\n",
    "#                 bbox_color = COLORMAP[4]\n",
    "#             elif v['role_name'] == 'autopilot':\n",
    "#                 text = 'veh'\n",
    "#                 bbox_color  = COLORMAP[10]        \n",
    "#             elif v['role_name'] == 'hero':\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 raise ValueError(\"Unexpected role: s\" % v['role_name'])\n",
    "\n",
    "#             cv2.rectangle(im_overlay, (umin, vmin), (umax, vmax), bbox_color, 2)\n",
    "#             cv2.putText(im_overlay, text, (umin+5, vmin+5), cv2.FONT_HERSHEY_SIMPLEX, 1, bbox_color, 2)\n",
    "\n",
    "plt.imshow(im_overlay)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlim([ego_x - 50., ego_x + 50.])\n",
    "plt.ylim([ego_y - 50., ego_y + 50.])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import carla\n",
    "for (k,v) in data_dicts[img_num].items():\n",
    "    if 'sensor' in k:\n",
    "        loc = carla.Location(x = v['location']['x'],\n",
    "                             y = v['location']['y'],\n",
    "                             z = v['location']['z'])\n",
    "        rot = carla.Rotation(pitch = v['rotation']['pitch'],\n",
    "                             yaw   = v['rotation']['yaw'],\n",
    "                             roll  = v['rotation']['roll'])\n",
    "        trans = carla.Transform(location=loc, rotation=rot)\n",
    "        \n",
    "        # trans.transform(): local frame => global/map frame\n",
    "        # trans.\n",
    "        \n",
    "        print(trans.location, trans.rotation)\n",
    "        print(dir(trans))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROGRESS\n",
    "* Shown how to convert segmentation/depth to a point cloud and project to top view.\n",
    "* \n",
    "\n",
    "#TODO:\n",
    "* 2D to 3D camera projection (need to get intrinsic matrix from json)\n",
    "* camera <-> world frame transformation\n",
    "* bbox plotting in world frame\n",
    "* association of bbox with projected points\n",
    "* 3D/2D bbox label for relevant objects (pedestrians, vehicles, stop/yield/speed signs, traffic lights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
